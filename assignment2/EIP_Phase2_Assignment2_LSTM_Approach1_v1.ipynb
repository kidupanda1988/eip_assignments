{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIP_Phase2_Assignment2_LSTM_Approach1_v1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5WrzsH3AZhS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bswgJHMzASmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4fvSUS4AUEs",
        "colab_type": "code",
        "outputId": "a9667894-5a25-4d05-84c5-e9174e51afb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trHHlWt8s5wI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEl3eS8PCkiQ",
        "colab_type": "code",
        "outputId": "32222e17-b1e8-4f1c-e35d-40f92606427f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1iai6zKxrGYIza4k1Kbwb8eTyD4wybYE8'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "print('Downloaded content \"{}\"'.format(downloaded.GetContentString()[0:500]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded content \"﻿CHAPTER I. Down the Rabbit-Hole\r\n",
            "\r\n",
            "Alice was beginning to get very tired of sitting by her sister on the\r\n",
            "bank, and of having nothing to do: once or twice she had peeped into the\r\n",
            "book her sister was reading, but it had no pictures or conversations in\r\n",
            "it, ‘and what is the use of a book,’ thought Alice ‘without pictures or\r\n",
            "conversations?’\r\n",
            "\r\n",
            "So she was considering in her own mind (as well as she could, for the\r\n",
            "hot day made her feel very sleepy and stupid), whether the pleasure\r\n",
            "of making a da\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV24UA8TlIim",
        "colab_type": "text"
      },
      "source": [
        "# How sequences are generated?  \n",
        "Instead of using a sliding window of a fixed size, we add more charcters to our sequence and ask the network to predict the next character.The sequences will be padded so that all of them will be of length 100.If there are sentences longer than 100 characters, then they will be broken down into smaller pieces and the above mentioned approah will be applied on each of the smaller pieces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blmom44ukcwH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "7fb11486-84ed-4e8b-dbf6-36bb59b470e5"
      },
      "source": [
        "sentence=\"mary had a little lamb\"\n",
        "for i in range(0,len(sentence)):\n",
        "  seq_in=sentence[0:i]\n",
        "  seq_out=sentence[i]\n",
        "  print('input:',seq_in,\".output:\",seq_out)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:  .output: m\n",
            "input: m .output: a\n",
            "input: ma .output: r\n",
            "input: mar .output: y\n",
            "input: mary .output:  \n",
            "input: mary  .output: h\n",
            "input: mary h .output: a\n",
            "input: mary ha .output: d\n",
            "input: mary had .output:  \n",
            "input: mary had  .output: a\n",
            "input: mary had a .output:  \n",
            "input: mary had a  .output: l\n",
            "input: mary had a l .output: i\n",
            "input: mary had a li .output: t\n",
            "input: mary had a lit .output: t\n",
            "input: mary had a litt .output: l\n",
            "input: mary had a littl .output: e\n",
            "input: mary had a little .output:  \n",
            "input: mary had a little  .output: l\n",
            "input: mary had a little l .output: a\n",
            "input: mary had a little la .output: m\n",
            "input: mary had a little lam .output: b\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMu_Q45dk0t_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODLdMa4DcCer",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Text Generation model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VCjkBnAtE4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import string\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class TextGenerator:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.clean_text=None\n",
        "    self.chars=None\n",
        "    self.char_to_int=None\n",
        "    self.sequence_model=None\n",
        "    self.X=None\n",
        "    self.y=None\n",
        "    \n",
        "    \n",
        "  def load_and_clean_text(self,file_content):\n",
        "    \n",
        "    #converting text to lower case\n",
        "    self.clean_text=file_content.lower()\n",
        "    \n",
        "    #strip all of the new line characters so that we have one long sequence of characters separated only by white space.\n",
        "    tokens=self.clean_text.split()\n",
        "    self.clean_text=' '.join(tokens)\n",
        "    \n",
        "    #removing punctuations other than full stop\n",
        "    punctuations_to_remove=''.join([ch for ch in string.punctuation if ch!='.'])\n",
        "    self.clean_text=self.clean_text.translate(str.maketrans('', '', punctuations_to_remove))\n",
        "    self.clean_text=self.clean_text.replace('\\ufeff',\" \").replace('\\n',\" \").replace('\\r',\" \").replace( '‘',\"\").replace('’',\"\").replace('“',\"\").replace('”',\"\").replace(\"  \",\" \")\n",
        "    \n",
        "    # create mapping of unique chars to integers\n",
        "    self.chars = sorted(list(set(self.clean_text)))\n",
        "    \n",
        "    self.char_to_int = dict((c, i) for i, c in enumerate(self.chars))\n",
        "    self.int_to_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "    \n",
        "    self.n_chars = len(self.clean_text)\n",
        "    self.n_vocab = len(self.chars)\n",
        "    print(\"Total Characters: \", self.n_chars)\n",
        "    print(\"Total Vocab: \", self.n_vocab)\n",
        "    \n",
        "    \n",
        "  def prepare_data_set(self,seq_len=100):\n",
        "    #prepare the dataset of input to output pairs encoded as integers\n",
        "    seq_length=seq_len\n",
        "    dataX=[]\n",
        "    dataY=[]\n",
        "    \n",
        "    sentences=self.clean_text.split('.')\n",
        "    \n",
        "    for sentence in sentences:\n",
        "      if len(sentence)<=seq_length:\n",
        "        for i in range(0,len(sentence)):\n",
        "          seq_in=sentence[0:i]\n",
        "          seq_out=sentence[i]\n",
        "          dataX.append([self.char_to_int[char] for char in seq_in])\n",
        "          dataY.append(self.char_to_int[seq_out])\n",
        "      else:\n",
        "        smaller_sentences=[sentence[i:i+seq_length] for i in range(0, len(sentence), seq_length)]\n",
        "        for smaller_sentence in smaller_sentences:\n",
        "          if len(smaller_sentence)<=seq_length:\n",
        "            for i in range(0,len(smaller_sentence)):\n",
        "              seq_in=smaller_sentence[0:i]\n",
        "              seq_out=smaller_sentence[i]\n",
        "              dataX.append([self.char_to_int[char] for char in seq_in])\n",
        "              dataY.append(self.char_to_int[seq_out])\n",
        "              \n",
        "      \n",
        "    n_patterns = len(dataX)\n",
        "    print(\"Total Patterns: \", n_patterns)\n",
        "    dataX_padded=np.array(pad_sequences(dataX, maxlen=seq_length, padding='pre'))\n",
        "    \n",
        "    print('shape of dataX_padded:',dataX_padded.shape)\n",
        "    # reshape X to be [samples, time steps, features]\n",
        "    self.X = np.reshape(dataX_padded, (n_patterns, seq_length, 1))\n",
        "    # normalize\n",
        "    self.X = self.X / float(self.n_vocab)\n",
        "    # one hot encode the output variable\n",
        "    self.y = np_utils.to_categorical(dataY)\n",
        "    \n",
        "    \n",
        "  def create_sequence_model(self,):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(256, input_shape=(self.X.shape[1],self.X.shape[2]),return_sequences=True))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(LSTM(256))\n",
        "    model.add(Dense(self.y.shape[1], activation='softmax'))\n",
        "    \n",
        "    \n",
        "    self.sequence_model=model\n",
        "    self.sequence_model.summary()\n",
        "    \n",
        "  def train_model(self,file_path_to_store_weights,num_epochs=100,batch_size=128):\n",
        "    \n",
        "    # Prepare callbacks for model saving and for learning rate adjustment.\n",
        "    checkpoint = ModelCheckpoint(filepath=file_path_to_store_weights,\n",
        "                             monitor='loss',\n",
        "                             verbose=1,\n",
        "                             save_weights_only=False,\n",
        "                             save_best_only=False,\n",
        "                             mode='min',\n",
        "                             period=10)\n",
        "    \n",
        "    callbacks_list = [checkpoint]\n",
        "    # fit the model\n",
        "    self.sequence_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    self.sequence_model.fit(self.X, self.y, epochs=num_epochs, batch_size=batch_size, callbacks=callbacks_list)\n",
        "    \n",
        "\n",
        "  def predict(self,char_seed='cat',num_chars_to_predict=100):\n",
        "    char_prediction=char_seed\n",
        "    pattern = [self.char_to_int[ch] for ch in char_seed]\n",
        "    print(\"Seed:\",char_seed)\n",
        "  \n",
        "    # generate characters\n",
        "    for i in range(num_chars_to_predict):\n",
        "      x = np.array(pad_sequences([pattern], maxlen=100, padding='pre'))\n",
        "      x=np.reshape(x, (1, 100, 1))\n",
        "      x = x / float(self.n_vocab)\n",
        "      prediction = self.sequence_model.predict(x, verbose=0)\n",
        "      index = np.argmax(prediction)\n",
        "      result = self.int_to_char[index]\n",
        "      seq_in = [self.int_to_char[value] for value in pattern]\n",
        "      char_prediction+=result\n",
        "      pattern.append(index)\n",
        "      pattern = pattern[1:len(pattern)]\n",
        "  \n",
        "    return char_prediction  \n",
        "    \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUw_yh8uC8sj",
        "colab_type": "code",
        "outputId": "0dd1aee2-6b68-44fc-f3d6-7d920da1fb48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator=TextGenerator()\n",
        "txt_generator.load_and_clean_text(downloaded.GetContentString())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  135043\n",
            "Total Vocab:  28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0B6-UYYqKx6",
        "colab_type": "code",
        "outputId": "383ab10d-a32f-4d2b-887d-c8c6f795691f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.prepare_data_set(seq_len=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  134054\n",
            "shape of dataX_padded: (134054, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOJCYjfYu9cc",
        "colab_type": "code",
        "outputId": "eaf09cbe-fad1-42b0-a569-d63e2970b109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "txt_generator.create_sequence_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0725 08:05:56.749824 140528753457024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0725 08:05:56.792500 140528753457024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0725 08:05:56.801749 140528753457024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0725 08:05:57.194788 140528753457024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0725 08:05:57.208194 140528753457024 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 28)                7196      \n",
            "=================================================================\n",
            "Total params: 796,700\n",
            "Trainable params: 796,700\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bH3XScbrjpd",
        "colab_type": "code",
        "outputId": "98369815-b379-4773-c045-97ef11891819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "weights_save_path='/content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1/July_23/epochs_001_100:{epoch:03d}.hdf5'\n",
        "txt_generator.train_model(file_path_to_store_weights=weights_save_path,num_epochs=100,batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "134054/134054 [==============================] - 147s 1ms/step - loss: 2.8182\n",
            "Epoch 2/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 2.6516\n",
            "Epoch 3/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 2.4167\n",
            "Epoch 4/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 2.2215\n",
            "Epoch 5/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 2.0871\n",
            "Epoch 6/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.9858\n",
            "Epoch 7/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.9019\n",
            "Epoch 8/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.8358\n",
            "Epoch 9/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.7776\n",
            "Epoch 10/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.7266\n",
            "\n",
            "Epoch 00010: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1/July_23/epochs_001_100:010.hdf5\n",
            "Epoch 11/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.6824\n",
            "Epoch 12/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.6431\n",
            "Epoch 13/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.6044\n",
            "Epoch 14/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.5740\n",
            "Epoch 15/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.5410\n",
            "Epoch 16/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.5107\n",
            "Epoch 17/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.4849\n",
            "Epoch 18/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.4581\n",
            "Epoch 19/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.4316\n",
            "Epoch 20/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.4086\n",
            "\n",
            "Epoch 00020: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1/July_23/epochs_001_100:020.hdf5\n",
            "Epoch 21/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.3859\n",
            "Epoch 22/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.3644\n",
            "Epoch 23/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.3422\n",
            "Epoch 24/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.3206\n",
            "Epoch 25/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.3011\n",
            "Epoch 26/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.2785\n",
            "Epoch 27/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.2643\n",
            "Epoch 28/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.2414\n",
            "Epoch 29/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.2233\n",
            "Epoch 30/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.2052\n",
            "\n",
            "Epoch 00030: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1/July_23/epochs_001_100:030.hdf5\n",
            "Epoch 31/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.1857\n",
            "Epoch 32/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.1681\n",
            "Epoch 33/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.1506\n",
            "Epoch 34/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.1341\n",
            "Epoch 35/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.1188\n",
            "Epoch 36/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.1007\n",
            "Epoch 37/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.0839\n",
            "Epoch 38/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.0680\n",
            "Epoch 39/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.0504\n",
            "Epoch 40/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.0349\n",
            "\n",
            "Epoch 00040: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1/July_23/epochs_001_100:040.hdf5\n",
            "Epoch 41/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.0160\n",
            "Epoch 42/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.0044\n",
            "Epoch 43/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.9897\n",
            "Epoch 44/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.9697\n",
            "Epoch 45/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.9556\n",
            "Epoch 46/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.9410\n",
            "Epoch 47/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.9281\n",
            "Epoch 48/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.9137\n",
            "Epoch 49/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.8972\n",
            "Epoch 50/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.8828\n",
            "\n",
            "Epoch 00050: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1/July_23/epochs_001_100:050.hdf5\n",
            "Epoch 51/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.8674\n",
            "Epoch 52/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.8562\n",
            "Epoch 53/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.8411\n",
            "Epoch 54/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.8292\n",
            "Epoch 55/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.8165\n",
            "Epoch 56/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.8035\n",
            "Epoch 57/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.7888\n",
            "Epoch 58/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.7756\n",
            "Epoch 59/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.7625\n",
            "Epoch 60/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.7540\n",
            "\n",
            "Epoch 00060: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1/July_23/epochs_001_100:060.hdf5\n",
            "Epoch 61/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.7417\n",
            "Epoch 62/100\n",
            "134054/134054 [==============================] - 146s 1ms/step - loss: 0.7315\n",
            "Epoch 63/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.7160\n",
            "Epoch 64/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.7049\n",
            "Epoch 65/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.6915\n",
            "Epoch 66/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.6857\n",
            "Epoch 67/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.6758\n",
            "Epoch 68/100\n",
            "134054/134054 [==============================] - 146s 1ms/step - loss: 0.6676\n",
            "Epoch 69/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.6497\n",
            "Epoch 70/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.6463\n",
            "\n",
            "Epoch 00070: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1/July_23/epochs_001_100:070.hdf5\n",
            "Epoch 71/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.6318\n",
            "Epoch 72/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.6298\n",
            "Epoch 73/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.6132\n",
            "Epoch 74/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.6078\n",
            "Epoch 75/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.5984\n",
            "Epoch 76/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.5886\n",
            "Epoch 77/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.5812\n",
            "Epoch 78/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.5776\n",
            "Epoch 79/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.5686\n",
            "Epoch 80/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.5559\n",
            "\n",
            "Epoch 00080: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1/July_23/epochs_001_100:080.hdf5\n",
            "Epoch 81/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.5481\n",
            "Epoch 82/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.5463\n",
            "Epoch 83/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.5334\n",
            "Epoch 84/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.5291\n",
            "Epoch 85/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.5249\n",
            "Epoch 86/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.5210\n",
            "Epoch 87/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.5075\n",
            "Epoch 88/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.5010\n",
            "Epoch 89/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.4963\n",
            "Epoch 90/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.4943\n",
            "\n",
            "Epoch 00090: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1/July_23/epochs_001_100:090.hdf5\n",
            "Epoch 91/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 0.4855\n",
            "Epoch 92/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.4820\n",
            "Epoch 93/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.4722\n",
            "Epoch 94/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.4662\n",
            "Epoch 95/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.4663\n",
            "Epoch 96/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.4579\n",
            "Epoch 97/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.4560\n",
            "Epoch 98/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.4493\n",
            "Epoch 99/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.4490\n",
            "Epoch 100/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 0.4342\n",
            "\n",
            "Epoch 00100: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1/July_23/epochs_001_100:100.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtBJfRNrKvxN",
        "colab_type": "text"
      },
      "source": [
        "# Predicting characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D68THJWAKzWc",
        "colab_type": "code",
        "outputId": "0143ff27-5319-4075-d2cd-b12c654dd2fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "txt_generator.create_sequence_model()\n",
        "#load weights from the 100th epoch\n",
        "txt_generator.sequence_model.load_weights(\"/content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1/July_23/epochs_001_100:100.hdf5\")\n",
        "txt_generator.sequence_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_5 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 28)                7196      \n",
            "=================================================================\n",
            "Total params: 796,700\n",
            "Trainable params: 796,700\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRoitmsJa6ad",
        "colab_type": "text"
      },
      "source": [
        "## Predicting 100 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2DscBlVLM6v",
        "colab_type": "code",
        "outputId": "2753b180-cafa-49c7-ba62-7692f7d04bf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.predict(char_seed='alice wants to',num_chars_to_predict=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: alice wants to\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'alice wants toom as she could remember that tar swil loot be in gllly shared the rueen still said the caterpillart'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWH-W_Wf9H8c",
        "colab_type": "code",
        "outputId": "0acf6bc4-9194-466d-d536-314c4741bc1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.predict(char_seed='he is mad',num_chars_to_predict=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: he is mad\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'he is madelt be hrrw erighfn seame tr a strrocsiness she whnhout abcourr the did of the games in she remeeten'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz47XFopLq17",
        "colab_type": "code",
        "outputId": "063abc3f-9fdf-4c9f-b7b7-a6f7349431a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.predict(char_seed='jon snow knows nothing',num_chars_to_predict=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: jon snow knows nothing\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'jon snow knows nothing marger alice said and wavier what said the caterpillar and the moment she and he wes you wouldnt ge'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HpKURK2L5GA",
        "colab_type": "code",
        "outputId": "adac3795-55f1-484c-8ce1-7217789c0e11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.predict(char_seed='a lannister always',num_chars_to_predict=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: a lannister always\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a lannister always get itee snder and whispered again and all to oe and raid to her ssoprs of the words drink me but n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIUhjrLJMAxE",
        "colab_type": "code",
        "outputId": "ba3e3f24-5927-47b6-f93f-b728a9b8b94e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.predict(char_seed='the meaning of life is',num_chars_to_predict=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: the meaning of life is\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the meaning of life is pl as the dormowse dound oot a very gurane somnow about this take she heard a lond whthout with its'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F02Plc5TNL5Q",
        "colab_type": "code",
        "outputId": "1e378171-3222-4726-b288-d636f9def26e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.predict(char_seed='twinkle twinkle',num_chars_to_predict=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: twinkle twinkle\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'twinkle twinkle his head tau onent of course i was i aegin with the noral of that does it may i sherle loter that s'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDaRhelVbE13",
        "colab_type": "text"
      },
      "source": [
        "## Predicting 500 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC74Q7XVNUuJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "97f1841e-0e74-43c6-d9b9-4fb9a5e32240"
      },
      "source": [
        "prediction=txt_generator.predict(char_seed='alice wants to',num_chars_to_predict=500)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: alice wants to\n",
            "alice wants toom as she could remember that tar swil loot be in gllly shared the rueen still said th\n",
            "e caterpillart totnde it myst mot what tork a oarral with the moral of that better tail when i eont \n",
            "see soogthedred so blm her vire aldrning as hi course down on the swenm of breadandbdyfragooe oerson\n",
            "ssiy present them and then all pever cl po mnw rersled for the shorld and then aett foid and she wen\n",
            "t on growing and she felt that she was allow wpok aetentakd different begind iis she samk of the gryphon rabbings \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5XR0cUZdPbR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "dc8b2138-c84a-4f85-ca48-0a6489027187"
      },
      "source": [
        "prediction=txt_generator.predict(char_seed='jon snow knows nothing',num_chars_to_predict=500)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: jon snow knows nothing\n",
            "jon snow knows nothing marger alice said and wavier what said the caterpillar and the moment she and\n",
            " he wes you wouldnt geet terdated something but she recsod toice and wton any more as all anice for \n",
            "inlw it to ilv that cane uime afain in simence in the night with at last she stretch harger and was \n",
            "belight it eilishing the words as shey all sereat down at the footman lnkee and mooking at he hande \n",
            "liceln byt of course you know what it was addatse the knave of hearts would tee to moen tortowfuloe would be taving and wa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ow6aXWfdr6S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9c511fd1-3370-473c-95be-7f31efd4c468"
      },
      "source": [
        "prediction=txt_generator.predict(char_seed='a lannister always',num_chars_to_predict=500)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: a lannister always\n",
            "a lannister always get itee snder and whispered again and all to oe and raid to her ssoprs of the wo\n",
            "rds drink me but nobusers alice remarked becuint a good was surp and but it said the mock turtle ten\n",
            "oered so surn into a pig all sound ht a moments decl id went nn uo iimking so food you know as the d\n",
            "oom tway wnur alice tolemlent adcidndnly ani as her the stajec being meft hfep what she was cown on \n",
            "the swenters thought alice to her and she trial down and walked two and hand in tay only reemvev and the mock turtle t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUj_RPfLcWT5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c8c5bfb0-6b2e-4287-85fc-f6903ae91d66"
      },
      "source": [
        "prediction=txt_generator.predict(char_seed='the meaning of life is',num_chars_to_predict=500)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: the meaning of life is\n",
            "the meaning of life is pl as the dormowse dound oot a very gurane somnow about this take she heard a\n",
            " lond whthout with its aegnne the roof was so swal ly hir sometfine worth her latser which was how t\n",
            "o get tuuef down with euryner sile the mock turtle angral to dar yhen she walted tas a mong will be \n",
            "santen in the siceer cruwp oo their heads down and said to the knave of course the gryphon repeated \n",
            "orines the only different and the other said to the great hulg tpy luch she were lived a boms little all fay to meaning oo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA4cuxYxbzEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
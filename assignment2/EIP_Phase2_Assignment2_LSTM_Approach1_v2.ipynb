{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIP_Phase2_Assignment2_LSTM_Approach1_v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5WrzsH3AZhS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bswgJHMzASmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4fvSUS4AUEs",
        "colab_type": "code",
        "outputId": "aa3d4e96-46f0-44c6-9974-08c5db8c6151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trHHlWt8s5wI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEl3eS8PCkiQ",
        "colab_type": "code",
        "outputId": "71dc54cf-fe4d-4546-8659-897d78634979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1iai6zKxrGYIza4k1Kbwb8eTyD4wybYE8'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "print('Downloaded content \"{}\"'.format(downloaded.GetContentString()[0:500]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded content \"﻿CHAPTER I. Down the Rabbit-Hole\r\n",
            "\r\n",
            "Alice was beginning to get very tired of sitting by her sister on the\r\n",
            "bank, and of having nothing to do: once or twice she had peeped into the\r\n",
            "book her sister was reading, but it had no pictures or conversations in\r\n",
            "it, ‘and what is the use of a book,’ thought Alice ‘without pictures or\r\n",
            "conversations?’\r\n",
            "\r\n",
            "So she was considering in her own mind (as well as she could, for the\r\n",
            "hot day made her feel very sleepy and stupid), whether the pleasure\r\n",
            "of making a da\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65_4MnWVmVIo",
        "colab_type": "text"
      },
      "source": [
        "# How sequences are generated?  \n",
        "Instead of using a sliding window of a fixed size, we add more charcters to our sequence and ask the network to predict the next character.The sequences will be padded so that all of them will be of length 100.If there are sentences longer than 100 characters, then they will be broken down into smaller pieces and the above mentioned approah will be applied on each of the smaller pieces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKq-Wy_hNPDt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "cb605a4e-a401-45ce-b2d5-4d08f03f7a2f"
      },
      "source": [
        "sentence=\"jon snow knows nothing\"\n",
        "for i in range(0,len(sentence)):\n",
        "  seq_in=sentence[0:i]\n",
        "  seq_out=sentence[i]\n",
        "  print('input:',seq_in,\".output:\",seq_out)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:  .output: j\n",
            "input: j .output: o\n",
            "input: jo .output: n\n",
            "input: jon .output:  \n",
            "input: jon  .output: s\n",
            "input: jon s .output: n\n",
            "input: jon sn .output: o\n",
            "input: jon sno .output: w\n",
            "input: jon snow .output:  \n",
            "input: jon snow  .output: k\n",
            "input: jon snow k .output: n\n",
            "input: jon snow kn .output: o\n",
            "input: jon snow kno .output: w\n",
            "input: jon snow know .output: s\n",
            "input: jon snow knows .output:  \n",
            "input: jon snow knows  .output: n\n",
            "input: jon snow knows n .output: o\n",
            "input: jon snow knows no .output: t\n",
            "input: jon snow knows not .output: h\n",
            "input: jon snow knows noth .output: i\n",
            "input: jon snow knows nothi .output: n\n",
            "input: jon snow knows nothin .output: g\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNa-iffxmeKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODLdMa4DcCer",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Text Generation model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VCjkBnAtE4m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bde5e6ff-cbcc-482d-c855-90ab48a3a5f3"
      },
      "source": [
        "import numpy as np\n",
        "import string\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class TextGenerator:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.clean_text=None\n",
        "    self.chars=None\n",
        "    self.char_to_int=None\n",
        "    self.sequence_model=None\n",
        "    self.X=None\n",
        "    self.y=None\n",
        "    self.train_history=None\n",
        "    \n",
        "    \n",
        "  def load_and_clean_text(self,file_content):\n",
        "    \n",
        "    #converting text to lower case\n",
        "    self.clean_text=file_content.lower()\n",
        "    \n",
        "    #strip all of the new line characters so that we have one long sequence of characters separated only by white space.\n",
        "    tokens=self.clean_text.split()\n",
        "    self.clean_text=' '.join(tokens)\n",
        "    \n",
        "    #removing punctuations other than full stop\n",
        "    punctuations_to_remove=''.join([ch for ch in string.punctuation if ch!='.'])\n",
        "    self.clean_text=self.clean_text.translate(str.maketrans('', '', punctuations_to_remove))\n",
        "    self.clean_text=self.clean_text.replace('\\ufeff',\" \").replace('\\n',\" \").replace('\\r',\" \").replace( '‘',\"\").replace('’',\"\").replace('“',\"\").replace('”',\"\").replace(\"  \",\" \")\n",
        "    \n",
        "    # create mapping of unique chars to integers\n",
        "    self.chars = sorted(list(set(self.clean_text)))\n",
        "    \n",
        "    self.char_to_int = dict((c, i) for i, c in enumerate(self.chars))\n",
        "    self.int_to_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "    \n",
        "    self.n_chars = len(self.clean_text)\n",
        "    self.n_vocab = len(self.chars)\n",
        "    print(\"Total Characters: \", self.n_chars)\n",
        "    print(\"Total Vocab: \", self.n_vocab)\n",
        "    \n",
        "    \n",
        "  def prepare_data_set(self,seq_len=100):\n",
        "    #prepare the dataset of input to output pairs encoded as integers\n",
        "    seq_length=seq_len\n",
        "    dataX=[]\n",
        "    dataY=[]\n",
        "    \n",
        "    sentences=self.clean_text.split('.')\n",
        "    \n",
        "    for sentence in sentences:\n",
        "      if len(sentence)<=seq_length:\n",
        "        for i in range(0,len(sentence)):\n",
        "          seq_in=sentence[0:i]\n",
        "          seq_out=sentence[i]\n",
        "          dataX.append([self.char_to_int[char] for char in seq_in])\n",
        "          dataY.append(self.char_to_int[seq_out])\n",
        "      else:\n",
        "        smaller_sentences=[sentence[i:i+seq_length] for i in range(0, len(sentence), seq_length)]\n",
        "        for smaller_sentence in smaller_sentences:\n",
        "          if len(smaller_sentence)<=seq_length:\n",
        "            for i in range(0,len(smaller_sentence)):\n",
        "              seq_in=smaller_sentence[0:i]\n",
        "              seq_out=smaller_sentence[i]\n",
        "              dataX.append([self.char_to_int[char] for char in seq_in])\n",
        "              dataY.append(self.char_to_int[seq_out])\n",
        "              \n",
        "      \n",
        "    n_patterns = len(dataX)\n",
        "    print(\"Total Patterns: \", n_patterns)\n",
        "    dataX_padded=np.array(pad_sequences(dataX, maxlen=seq_length, padding='pre'))\n",
        "    \n",
        "    print('shape of dataX_padded:',dataX_padded.shape)\n",
        "    # reshape X to be [samples, time steps, features]\n",
        "    self.X = np.reshape(dataX_padded, (n_patterns, seq_length, 1))\n",
        "    # normalize\n",
        "    self.X = self.X / float(self.n_vocab)\n",
        "    # one hot encode the output variable\n",
        "    self.y = np_utils.to_categorical(dataY)\n",
        "    \n",
        "    \n",
        "  def create_sequence_model(self,num_lstm_units=256,drop_out=0.1):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(num_lstm_units, input_shape=(self.X.shape[1],self.X.shape[2]),return_sequences=True))\n",
        "    model.add(Dropout(drop_out))\n",
        "    model.add(LSTM(num_lstm_units))\n",
        "    model.add(Dense(self.y.shape[1], activation='softmax'))\n",
        "    \n",
        "    self.sequence_model=model\n",
        "    self.sequence_model.summary()\n",
        "    \n",
        "  def train_model(self,file_path_to_store_weights,\n",
        "                  num_epochs=100,\n",
        "                  batch_size=128,\n",
        "                 loss_fn='categorical_crossentropy',\n",
        "                 optimizer_name='adam'):\n",
        "    \n",
        "    # Prepare callbacks for model saving and for learning rate adjustment.\n",
        "    checkpoint = ModelCheckpoint(filepath=file_path_to_store_weights,\n",
        "                             monitor='loss',\n",
        "                             verbose=1,\n",
        "                             save_weights_only=False,\n",
        "                             save_best_only=False,\n",
        "                             mode='min',\n",
        "                             period=10)\n",
        "    \n",
        "    callbacks_list = [checkpoint]\n",
        "    \n",
        "    self.sequence_model.compile(loss=loss_fn, optimizer=optimizer_name)\n",
        "    # fit the model\n",
        "    self.train_history=self.sequence_model.fit(self.X, self.y, epochs=num_epochs, batch_size=batch_size, callbacks=callbacks_list)\n",
        "    \n",
        "\n",
        "  def predict(self,char_seed='cat',num_chars_to_predict=100):\n",
        "    char_prediction=char_seed\n",
        "    pattern = [self.char_to_int[ch] for ch in char_seed]\n",
        "    print(\"Seed:\",char_seed)\n",
        "  \n",
        "    # generate characters\n",
        "    for i in range(num_chars_to_predict):\n",
        "      x = np.array(pad_sequences([pattern], maxlen=100, padding='pre'))\n",
        "      x=np.reshape(x, (1, 100, 1))\n",
        "      x = x / float(self.n_vocab)\n",
        "      prediction = self.sequence_model.predict(x, verbose=0)\n",
        "      index = np.argmax(prediction)\n",
        "      result = self.int_to_char[index]\n",
        "      seq_in = [self.int_to_char[value] for value in pattern]\n",
        "      char_prediction+=result\n",
        "      pattern.append(index)\n",
        "      pattern = pattern[1:len(pattern)]\n",
        "  \n",
        "    return char_prediction  \n",
        "  "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUw_yh8uC8sj",
        "colab_type": "code",
        "outputId": "37ac9781-d9e9-42ea-9346-0b29b01e679e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator=TextGenerator()\n",
        "txt_generator.load_and_clean_text(downloaded.GetContentString())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  135043\n",
            "Total Vocab:  28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0B6-UYYqKx6",
        "colab_type": "code",
        "outputId": "364a0117-6af1-46a1-e3e9-ac8a5b5e2587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.prepare_data_set(seq_len=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  134054\n",
            "shape of dataX_padded: (134054, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOJCYjfYu9cc",
        "colab_type": "code",
        "outputId": "f8f6f50d-715c-4152-e410-a76191032b33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "txt_generator.create_sequence_model(num_lstm_units=256,drop_out=0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0724 09:28:18.569561 140232650028928 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0724 09:28:18.590406 140232650028928 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0724 09:28:18.593562 140232650028928 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0724 09:28:18.948723 140232650028928 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0724 09:28:18.963500 140232650028928 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 28)                7196      \n",
            "=================================================================\n",
            "Total params: 796,700\n",
            "Trainable params: 796,700\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bH3XScbrjpd",
        "colab_type": "code",
        "outputId": "427555a2-101c-44ec-a2c1-36cb3e697008",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "weights_save_path='/content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v2/July_24/epochs_001_100:{epoch:03d}.hdf5'\n",
        "txt_generator.train_model(file_path_to_store_weights=weights_save_path,num_epochs=100,batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0724 09:28:34.497663 140232650028928 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0724 09:28:34.539849 140232650028928 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0724 09:28:34.693741 140232650028928 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 2.8168\n",
            "Epoch 2/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 2.6423\n",
            "Epoch 3/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 2.4243\n",
            "Epoch 4/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 2.2613\n",
            "Epoch 5/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 2.1344\n",
            "Epoch 6/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 2.0391\n",
            "Epoch 7/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.9624\n",
            "Epoch 8/100\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 1.8995\n",
            "Epoch 9/100\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 1.8427\n",
            "Epoch 10/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.7939\n",
            "\n",
            "Epoch 00010: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v2/July_24/epochs_001_100:010.hdf5\n",
            "Epoch 11/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.7520\n",
            "Epoch 12/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.7116\n",
            "Epoch 13/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.6778\n",
            "Epoch 14/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.6428\n",
            "Epoch 15/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.6124\n",
            "Epoch 16/100\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 1.5837\n",
            "Epoch 17/100\n",
            "134054/134054 [==============================] - 141s 1ms/step - loss: 1.5556\n",
            "Epoch 18/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.5299\n",
            "Epoch 19/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.5061\n",
            "Epoch 20/100\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 1.4823\n",
            "\n",
            "Epoch 00020: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v2/July_24/epochs_001_100:020.hdf5\n",
            "Epoch 21/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.4591\n",
            "Epoch 22/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.4371\n",
            "Epoch 23/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.4154\n",
            "Epoch 24/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.3957\n",
            "Epoch 25/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.3730\n",
            "Epoch 26/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.3531\n",
            "Epoch 27/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.3355\n",
            "Epoch 28/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.3148\n",
            "Epoch 29/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.2991\n",
            "Epoch 30/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.2813\n",
            "\n",
            "Epoch 00030: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v2/July_24/epochs_001_100:030.hdf5\n",
            "Epoch 31/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.2621\n",
            "Epoch 32/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.2452\n",
            "Epoch 33/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.2313\n",
            "Epoch 34/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.2122\n",
            "Epoch 35/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.1982\n",
            "Epoch 36/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.1874\n",
            "Epoch 37/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.1682\n",
            "Epoch 38/100\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 1.1538\n",
            "Epoch 39/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.1367\n",
            "Epoch 40/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.1246\n",
            "\n",
            "Epoch 00040: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v2/July_24/epochs_001_100:040.hdf5\n",
            "Epoch 41/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.1117\n",
            "Epoch 42/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 1.0980\n",
            "Epoch 43/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.0869\n",
            "Epoch 44/100\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 1.0721\n",
            "Epoch 45/100\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 1.0614\n",
            "Epoch 46/100\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 1.0479\n",
            "Epoch 47/100\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 1.0364\n",
            "Epoch 48/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.0247\n",
            "Epoch 49/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.0138\n",
            "Epoch 50/100\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 1.0040\n",
            "\n",
            "Epoch 00050: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v2/July_24/epochs_001_100:050.hdf5\n",
            "Epoch 51/100\n",
            " 42496/134054 [========>.....................] - ETA: 1:37 - loss: 0.9710"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5e46dddc92d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mweights_save_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v2/July_24/epochs_001_100:{epoch:03d}.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtxt_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path_to_store_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_save_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-20bf5e92823c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, file_path_to_store_weights, num_epochs, batch_size, loss_fn, optimizer_name)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AI_IaKova99",
        "colab_type": "text"
      },
      "source": [
        "Epochs 50 to 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT62Zuo2QoWf",
        "colab_type": "code",
        "outputId": "8d84e0ce-49bf-452d-c86d-bddcbe274e07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator=TextGenerator()\n",
        "txt_generator.load_and_clean_text(downloaded.GetContentString())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  135043\n",
            "Total Vocab:  28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiA9O3QlvlEm",
        "colab_type": "code",
        "outputId": "b518cdac-06d1-43d6-d31f-8d221b245a5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.prepare_data_set(seq_len=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  134054\n",
            "shape of dataX_padded: (134054, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hlkjdP0vlIf",
        "colab_type": "code",
        "outputId": "5da8f204-ce59-4214-a231-7398eb56180c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "txt_generator.create_sequence_model(num_lstm_units=256,drop_out=0.5)\n",
        "txt_generator.sequence_model.load_weights(\"/content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v2/July_24/epochs_001_100:050.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_5 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 28)                7196      \n",
            "=================================================================\n",
            "Total params: 796,700\n",
            "Trainable params: 796,700\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVFBv1zHwKj-",
        "colab_type": "code",
        "outputId": "1d532efe-ce72-46bd-eb2f-a516ee564ad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "weights_save_path='/content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v2/July_24/epochs_051_100:{epoch:03d}.hdf5'\n",
        "txt_generator.train_model(file_path_to_store_weights=weights_save_path,num_epochs=50,batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "134054/134054 [==============================] - 146s 1ms/step - loss: 0.9942\n",
            "Epoch 2/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.9773\n",
            "Epoch 3/50\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 0.9689\n",
            "Epoch 4/50\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 0.9614\n",
            "Epoch 5/50\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 0.9498\n",
            "Epoch 6/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.9405\n",
            "Epoch 7/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.9295\n",
            "Epoch 8/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.9230\n",
            "Epoch 9/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.9159\n",
            "Epoch 10/50\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 0.9089\n",
            "\n",
            "Epoch 00010: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v2/July_24/epochs_051_100:010.hdf5\n",
            "Epoch 11/50\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 0.8976\n",
            "Epoch 12/50\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 0.8872\n",
            "Epoch 13/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.8831\n",
            "Epoch 14/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.8733\n",
            "Epoch 15/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.8689\n",
            "Epoch 16/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.8613\n",
            "Epoch 17/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.8511\n",
            "Epoch 18/50\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 0.8484\n",
            "Epoch 19/50\n",
            "134054/134054 [==============================] - 141s 1ms/step - loss: 0.8390\n",
            "Epoch 20/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.8339\n",
            "\n",
            "Epoch 00020: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v2/July_24/epochs_051_100:020.hdf5\n",
            "Epoch 21/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.8243\n",
            "Epoch 22/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.8182\n",
            "Epoch 23/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.8126\n",
            "Epoch 24/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.8038\n",
            "Epoch 25/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.7996\n",
            "Epoch 26/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.7948\n",
            "Epoch 27/50\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 0.7871\n",
            "Epoch 28/50\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 0.7844\n",
            "Epoch 29/50\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 0.7752\n",
            "Epoch 30/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.7677\n",
            "\n",
            "Epoch 00030: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v2/July_24/epochs_051_100:030.hdf5\n",
            "Epoch 31/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.7646\n",
            "Epoch 32/50\n",
            "134054/134054 [==============================] - 141s 1ms/step - loss: 0.7575\n",
            "Epoch 33/50\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 0.7561\n",
            "Epoch 34/50\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 0.7482\n",
            "Epoch 35/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.7461\n",
            "Epoch 36/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.7407\n",
            "Epoch 37/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.7312\n",
            "Epoch 38/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.7298\n",
            "Epoch 39/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.7196\n",
            "Epoch 40/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.7164\n",
            "\n",
            "Epoch 00040: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v2/July_24/epochs_051_100:040.hdf5\n",
            "Epoch 41/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.7142\n",
            "Epoch 42/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.7120\n",
            "Epoch 43/50\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 0.7093\n",
            "Epoch 44/50\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 0.7040\n",
            "Epoch 45/50\n",
            "134054/134054 [==============================] - 142s 1ms/step - loss: 0.6968\n",
            "Epoch 46/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.6927\n",
            "Epoch 47/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.6890\n",
            "Epoch 48/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.6879\n",
            "Epoch 49/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.6860\n",
            "Epoch 50/50\n",
            "134054/134054 [==============================] - 143s 1ms/step - loss: 0.6796\n",
            "\n",
            "Epoch 00050: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v2/July_24/epochs_051_100:050.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lxh67v1HfAUR",
        "colab_type": "text"
      },
      "source": [
        "## Predicting characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aibulx7yfEcz",
        "colab_type": "text"
      },
      "source": [
        "## Predicting 100 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUDPCH8RfDsC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "19beda83-7b23-43b2-a287-ca1f9760bd02"
      },
      "source": [
        "txt_generator=TextGenerator()\n",
        "txt_generator.load_and_clean_text(downloaded.GetContentString())\n",
        "txt_generator.prepare_data_set(seq_len=100)\n",
        "txt_generator.create_sequence_model(num_lstm_units=256,drop_out=0.5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  135043\n",
            "Total Vocab:  28\n",
            "Total Patterns:  134054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0725 09:39:14.152803 140615828289408 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0725 09:39:14.158828 140615828289408 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "shape of dataX_padded: (134054, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0725 09:39:14.520613 140615828289408 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0725 09:39:14.532174 140615828289408 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 28)                7196      \n",
            "=================================================================\n",
            "Total params: 796,700\n",
            "Trainable params: 796,700\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZxVEkMzf6XZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "ee5d3d5b-3f80-4794-89a9-625cf749d260"
      },
      "source": [
        "txt_generator.sequence_model.load_weights(\"/content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v2/July_24/epochs_051_100:050.hdf5\")\n",
        "txt_generator.sequence_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0725 09:39:21.462977 140615828289408 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0725 09:39:22.083986 140615828289408 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3PKTbhaqMHm",
        "colab_type": "code",
        "outputId": "ce3b9130-bfb2-407e-e665-1ba406baf95a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.predict(char_seed='l',num_chars_to_predict=100)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: l\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lonononononononononononononononononononononononononononononononononononononononononononononononononon'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt-lN7NtdXY1",
        "colab_type": "code",
        "outputId": "aee9c8c8-b5a3-42a7-826a-3acccac69709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.predict(char_seed='cat',num_chars_to_predict=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: cat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cats mot nnt she what she what she what she what she what she what she what she what she what she what '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw46nzMn8nYS",
        "colab_type": "code",
        "outputId": "607e5a06-fa6a-47b1-dfe0-e13e56119ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.predict(char_seed='alice',num_chars_to_predict=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: alice\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'alice yoite tertentsnatlinpintseats the mooentinuteater the mooentinuteater the mooentinuteater the mooen'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbk-1cZ3FDFe",
        "colab_type": "code",
        "outputId": "4a15f882-121c-43c9-b046-d55a5c5a8aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.predict(char_seed='alice wants to',num_chars_to_predict=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: alice wants to\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'alice wants to the dance with a cound not said alice ald the mock turtle seem to har she said to herself io a hord'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psjeSeUQjhJt",
        "colab_type": "code",
        "outputId": "dbf4ca3b-4181-407f-f3a0-2618c0baf609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.predict(char_seed='cat was smoking a',num_chars_to_predict=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: cat was smoking a\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cat was smoking along the doom and a large cat in say in my youth said his face and raid fveryelbilly and wery soon f'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPxvCzvhlMRu",
        "colab_type": "code",
        "outputId": "b9460b79-2789-4632-bd83-be606444f4ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.predict(char_seed='the red queen was so angry',num_chars_to_predict=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: the red queen was so angry\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the red queen was so angry as the door with a soll foruer har and alice looked round eamcr a little pattering of feet in the d'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrNF5JIXlZnb",
        "colab_type": "code",
        "outputId": "0a3a8cf2-0324-460a-b6b8-a8fd9c1bc788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.predict(char_seed='knock knock',num_chars_to_predict=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: knock knock\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'knock knocked ay the enrmouse that it was the white rabbithole neal them she went on alice said not such a nuch'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCCTKCxAgIMl",
        "colab_type": "text"
      },
      "source": [
        "## Predicting 500 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBmkI3GKgLwU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bd4b7c01-0223-443b-a438-0dc15af1bc87"
      },
      "source": [
        "prediction=txt_generator.predict(char_seed='alice wants to',num_chars_to_predict=500)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: alice wants to\n",
            "alice wants to the dance with a cound not said alice ald the mock turtle seem to har she said to her\n",
            "self io a hord out a rmund it sooktilg down ttah a cuecen wiilg the reason the peaderhss spleshing a\n",
            "laie and the mock turtle seem to har she said to herself io a hord out a rmund it sooktilg down ttah\n",
            " a cuecen wiilg the reason the peaderhss spleshing alaie and the mock turtle seem to har she said to\n",
            " herself io a hord out a rmund it sooktilg down ttah a cuecen wiilg the reason the peaderhss spleshing alaie and t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52aKq0NSgf3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6f545aae-cb41-4faa-e478-624f01d9b1d5"
      },
      "source": [
        "prediction=txt_generator.predict(char_seed='jon snow knows nothing',num_chars_to_predict=500)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: jon snow knows nothing\n",
            "jon snow knows nothing eeat it sather a frcs at well as she could for the fod of the trial dar bette\n",
            "r now his fisst she said to herself in cand one feel cear so she went on all the sat down a large pa\n",
            "bbits were gladrels eoot the walked sather the dourt and the mock turtle said to the duchess and the\n",
            " mocster was a tery diffirotfd of the sane she wery seldmbering wety gnadendhing the queen said the \n",
            "doorman in a parce herself so see iow shan porse ar all the rabbit surtles the walked sather this outping becide she way a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu0LPk4sgi9d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "afd99699-6994-48f1-93ba-9f85d6857c36"
      },
      "source": [
        "prediction=txt_generator.predict(char_seed='a lannister always',num_chars_to_predict=500)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: a lannister always\n",
            "a lannister always get to say the mouse who seemed to be no courtered thought alice without becedent\n",
            "hduieyamd yhat you may gat betadle a baw uaid the duchess the rueen said the duchess as the doumouse\n",
            " looked all round the hatter id you dont know what the mouse goo fighing for a minute or two she was\n",
            " tilence thinking i should the tort of the thimgs is wer she said to herself which way abdeais puite\n",
            " fongotounsy how five to say when i was a little dook and the mock turtle taid alice vhought alice thought it would be\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzxV5ba-gmPM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d2e6512d-798a-438f-c2ea-da522b5ec05b"
      },
      "source": [
        "prediction=txt_generator.predict(char_seed='the meaning of life is',num_chars_to_predict=500)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: the meaning of life is\n",
            "the meaning of life is belongs to a mouse shat came vpon a little door kook at well as she could for\n",
            " the fod of the trial dar better now his fisst she said to herself in cand one feel cear so she went\n",
            " on all the sat down a large pabbits were gladrels eoot the walked sather the dourt and the mock tur\n",
            "tle said to the duchess and the mocster was a tery diffirotfd of the sane she wery seldmbering wety \n",
            "gnadendhing the queen said the doorman in a parce herself so see iow shan porse ar all the rabbit surtles the walked sathe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ines19Ogp8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi9HY_iglc5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIP_Phase2_Assignment2_LSTM_Approach1_v4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5WrzsH3AZhS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bswgJHMzASmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4fvSUS4AUEs",
        "colab_type": "code",
        "outputId": "cfdec826-f667-4a3a-bc77-4a17118edf1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trHHlWt8s5wI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEl3eS8PCkiQ",
        "colab_type": "code",
        "outputId": "384d6efd-f636-490a-8e5e-b13cc7eeeade",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1iai6zKxrGYIza4k1Kbwb8eTyD4wybYE8'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "print('Downloaded content \"{}\"'.format(downloaded.GetContentString()[0:500]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded content \"﻿CHAPTER I. Down the Rabbit-Hole\r\n",
            "\r\n",
            "Alice was beginning to get very tired of sitting by her sister on the\r\n",
            "bank, and of having nothing to do: once or twice she had peeped into the\r\n",
            "book her sister was reading, but it had no pictures or conversations in\r\n",
            "it, ‘and what is the use of a book,’ thought Alice ‘without pictures or\r\n",
            "conversations?’\r\n",
            "\r\n",
            "So she was considering in her own mind (as well as she could, for the\r\n",
            "hot day made her feel very sleepy and stupid), whether the pleasure\r\n",
            "of making a da\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65_4MnWVmVIo",
        "colab_type": "text"
      },
      "source": [
        "# How sequences are generated?  \n",
        "Instead of using a sliding window of a fixed size, we add more charcters to our sequence and ask the network to predict the next character.The sequences will be padded so that all of them will be of length 100.If there are sentences longer than 100 characters, then they will be broken down into smaller pieces and the above mentioned approah will be applied on each of the smaller pieces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKq-Wy_hNPDt",
        "colab_type": "code",
        "outputId": "2a1fbaef-e882-4b5c-b59a-0b1237cedb27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "sentence=\"jon snow knows nothing\"\n",
        "for i in range(0,len(sentence)):\n",
        "  seq_in=sentence[0:i]\n",
        "  seq_out=sentence[i]\n",
        "  print('input:',seq_in,\".output:\",seq_out)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:  .output: j\n",
            "input: j .output: o\n",
            "input: jo .output: n\n",
            "input: jon .output:  \n",
            "input: jon  .output: s\n",
            "input: jon s .output: n\n",
            "input: jon sn .output: o\n",
            "input: jon sno .output: w\n",
            "input: jon snow .output:  \n",
            "input: jon snow  .output: k\n",
            "input: jon snow k .output: n\n",
            "input: jon snow kn .output: o\n",
            "input: jon snow kno .output: w\n",
            "input: jon snow know .output: s\n",
            "input: jon snow knows .output:  \n",
            "input: jon snow knows  .output: n\n",
            "input: jon snow knows n .output: o\n",
            "input: jon snow knows no .output: t\n",
            "input: jon snow knows not .output: h\n",
            "input: jon snow knows noth .output: i\n",
            "input: jon snow knows nothi .output: n\n",
            "input: jon snow knows nothin .output: g\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNa-iffxmeKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODLdMa4DcCer",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Text Generation model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VCjkBnAtE4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import string\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class TextGenerator:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.clean_text=None\n",
        "    self.chars=None\n",
        "    self.char_to_int=None\n",
        "    self.sequence_model=None\n",
        "    self.X=None\n",
        "    self.y=None\n",
        "    self.train_history=None\n",
        "    \n",
        "    \n",
        "  def load_and_clean_text(self,file_content):\n",
        "    \n",
        "    #converting text to lower case\n",
        "    self.clean_text=file_content.lower()\n",
        "    \n",
        "    #strip all of the new line characters so that we have one long sequence of characters separated only by white space.\n",
        "    tokens=self.clean_text.split()\n",
        "    self.clean_text=' '.join(tokens)\n",
        "    \n",
        "    #removing punctuations other than full stop\n",
        "    punctuations_to_remove=''.join([ch for ch in string.punctuation if ch!='.'])\n",
        "    self.clean_text=self.clean_text.translate(str.maketrans('', '', punctuations_to_remove))\n",
        "    self.clean_text=self.clean_text.replace('\\ufeff',\" \").replace('\\n',\" \").replace('\\r',\" \").replace( '‘',\"\").replace('’',\"\").replace('“',\"\").replace('”',\"\").replace(\"  \",\" \")\n",
        "    \n",
        "    # create mapping of unique chars to integers\n",
        "    self.chars = sorted(list(set(self.clean_text)))\n",
        "    \n",
        "    self.char_to_int = dict((c, i) for i, c in enumerate(self.chars))\n",
        "    self.int_to_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "    \n",
        "    self.n_chars = len(self.clean_text)\n",
        "    self.n_vocab = len(self.chars)\n",
        "    print(\"Total Characters: \", self.n_chars)\n",
        "    print(\"Total Vocab: \", self.n_vocab)\n",
        "    \n",
        "    \n",
        "  def prepare_data_set(self,seq_len=100):\n",
        "    #prepare the dataset of input to output pairs encoded as integers\n",
        "    seq_length=seq_len\n",
        "    dataX=[]\n",
        "    dataY=[]\n",
        "    \n",
        "    sentences=self.clean_text.split('.')\n",
        "    \n",
        "    for sentence in sentences:\n",
        "      if len(sentence)<=seq_length:\n",
        "        for i in range(0,len(sentence)):\n",
        "          seq_in=sentence[0:i]\n",
        "          seq_out=sentence[i]\n",
        "          dataX.append([self.char_to_int[char] for char in seq_in])\n",
        "          dataY.append(self.char_to_int[seq_out])\n",
        "      else:\n",
        "        smaller_sentences=[sentence[i:i+seq_length] for i in range(0, len(sentence), seq_length)]\n",
        "        for smaller_sentence in smaller_sentences:\n",
        "          if len(smaller_sentence)<=seq_length:\n",
        "            for i in range(0,len(smaller_sentence)):\n",
        "              seq_in=smaller_sentence[0:i]\n",
        "              seq_out=smaller_sentence[i]\n",
        "              dataX.append([self.char_to_int[char] for char in seq_in])\n",
        "              dataY.append(self.char_to_int[seq_out])\n",
        "              \n",
        "      \n",
        "    n_patterns = len(dataX)\n",
        "    print(\"Total Patterns: \", n_patterns)\n",
        "    dataX_padded=np.array(pad_sequences(dataX, maxlen=seq_length, padding='pre'))\n",
        "    \n",
        "    print('shape of dataX_padded:',dataX_padded.shape)\n",
        "    # reshape X to be [samples, time steps, features]\n",
        "    self.X = np.reshape(dataX_padded, (n_patterns, seq_length, 1))\n",
        "    # normalize\n",
        "    self.X = self.X / float(self.n_vocab)\n",
        "    # one hot encode the output variable\n",
        "    self.y = np_utils.to_categorical(dataY)\n",
        "    \n",
        "    \n",
        "  def create_sequence_model(self,num_lstm_units=256,drop_out=0.1):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(num_lstm_units, input_shape=(self.X.shape[1],self.X.shape[2]),dropout=drop_out,return_sequences=True))\n",
        "    model.add(LSTM(num_lstm_units,dropout=drop_out))\n",
        "    model.add(Dense(self.y.shape[1], activation='softmax'))\n",
        "    \n",
        "    self.sequence_model=model\n",
        "    self.sequence_model.summary()\n",
        "    \n",
        "  def train_model(self,file_path_to_store_weights,\n",
        "                  num_epochs=100,\n",
        "                  batch_size=128,\n",
        "                 loss_fn='categorical_crossentropy',\n",
        "                 optimizer_name='adam'):\n",
        "    \n",
        "    # Prepare callbacks for model saving and for learning rate adjustment.\n",
        "    checkpoint = ModelCheckpoint(filepath=file_path_to_store_weights,\n",
        "                             monitor='loss',\n",
        "                             verbose=1,\n",
        "                             save_weights_only=False,\n",
        "                             save_best_only=False,\n",
        "                             mode='min',\n",
        "                             period=10)\n",
        "    \n",
        "    callbacks_list = [checkpoint]\n",
        "    \n",
        "    self.sequence_model.compile(loss=loss_fn, optimizer=optimizer_name)\n",
        "    # fit the model\n",
        "    self.train_history=self.sequence_model.fit(self.X, self.y, epochs=num_epochs, batch_size=batch_size, callbacks=callbacks_list)\n",
        "    \n",
        "\n",
        "  def predict(self,char_seed='cat',num_chars_to_predict=100):\n",
        "    char_prediction=char_seed\n",
        "    pattern = [self.char_to_int[ch] for ch in char_seed]\n",
        "    print(\"Seed:\",char_seed)\n",
        "  \n",
        "    # generate characters\n",
        "    for i in range(num_chars_to_predict):\n",
        "      x = np.array(pad_sequences([pattern], maxlen=100, padding='pre'))\n",
        "      x=np.reshape(x, (1, 100, 1))\n",
        "      x = x / float(self.n_vocab)\n",
        "      prediction = self.sequence_model.predict(x, verbose=0)\n",
        "      index = np.argmax(prediction)\n",
        "      result = self.int_to_char[index]\n",
        "      seq_in = [self.int_to_char[value] for value in pattern]\n",
        "      char_prediction+=result\n",
        "      pattern.append(index)\n",
        "      pattern = pattern[1:len(pattern)]\n",
        "  \n",
        "    return char_prediction  \n",
        "  \n",
        "  \n",
        "  def predict_with_temperature(self,char_seed='cat',num_chars_to_predict=100,temperature=1):\n",
        "    char_prediction=char_seed\n",
        "    pattern = [self.char_to_int[ch] for ch in char_seed]\n",
        "#     print(\"Seed:\",char_seed)\n",
        "  \n",
        "    # generate characters\n",
        "    for i in range(num_chars_to_predict):\n",
        "      x = np.array(pad_sequences([pattern], maxlen=100, padding='pre'))\n",
        "      x=np.reshape(x, (1, 100, 1))\n",
        "      x = x / float(self.n_vocab)\n",
        "      prediction = self.sequence_model.predict(x, verbose=0)\n",
        "      \n",
        "      preds=np.asarray(prediction).astype('float64')\n",
        "      preds=np.log(preds)/temperature\n",
        "      index = np.argmax(preds)\n",
        "      \n",
        "      result = self.int_to_char[index]\n",
        "      seq_in = [self.int_to_char[value] for value in pattern]\n",
        "      char_prediction+=result\n",
        "      pattern.append(index)\n",
        "      pattern = pattern[1:len(pattern)]\n",
        "  \n",
        "    return char_prediction \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUw_yh8uC8sj",
        "colab_type": "code",
        "outputId": "d483dff3-35ba-413b-ae95-b7ff0fad75d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator=TextGenerator()\n",
        "txt_generator.load_and_clean_text(downloaded.GetContentString())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  135043\n",
            "Total Vocab:  28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0B6-UYYqKx6",
        "colab_type": "code",
        "outputId": "80fd58b1-d4e9-4d5b-e6c8-a11e71b4c6f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt_generator.prepare_data_set(seq_len=100)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  134054\n",
            "shape of dataX_padded: (134054, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOJCYjfYu9cc",
        "colab_type": "code",
        "outputId": "6fbea73c-f669-45aa-82a9-0ce7f6c8ce27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "txt_generator.create_sequence_model(num_lstm_units=256,drop_out=0.1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_3 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 28)                7196      \n",
            "=================================================================\n",
            "Total params: 796,700\n",
            "Trainable params: 796,700\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bH3XScbrjpd",
        "colab_type": "code",
        "outputId": "d46adaa7-bdeb-41a8-f343-46bf1d821143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "weights_save_path='/content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v4/July_26/epochs_001_100:{epoch:03d}.hdf5'\n",
        "txt_generator.train_model(file_path_to_store_weights=weights_save_path,num_epochs=100,batch_size=512)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "134054/134054 [==============================] - 304s 2ms/step - loss: 2.8101\n",
            "Epoch 2/100\n",
            "134054/134054 [==============================] - 302s 2ms/step - loss: 2.6236\n",
            "Epoch 3/100\n",
            "134054/134054 [==============================] - 302s 2ms/step - loss: 2.4187\n",
            "Epoch 4/100\n",
            "134054/134054 [==============================] - 301s 2ms/step - loss: 2.2527\n",
            "Epoch 5/100\n",
            "134054/134054 [==============================] - 302s 2ms/step - loss: 2.1316\n",
            "Epoch 6/100\n",
            "134054/134054 [==============================] - 302s 2ms/step - loss: 2.0451\n",
            "Epoch 7/100\n",
            "134054/134054 [==============================] - 305s 2ms/step - loss: 1.9673\n",
            "Epoch 8/100\n",
            "134054/134054 [==============================] - 306s 2ms/step - loss: 1.9058\n",
            "Epoch 9/100\n",
            "134054/134054 [==============================] - 305s 2ms/step - loss: 1.8503\n",
            "Epoch 10/100\n",
            "134054/134054 [==============================] - 302s 2ms/step - loss: 1.8013\n",
            "\n",
            "Epoch 00010: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v4/July_26/epochs_001_100:010.hdf5\n",
            "Epoch 11/100\n",
            "134054/134054 [==============================] - 302s 2ms/step - loss: 1.7608\n",
            "Epoch 12/100\n",
            "134054/134054 [==============================] - 302s 2ms/step - loss: 1.7189\n",
            "Epoch 13/100\n",
            "134054/134054 [==============================] - 279s 2ms/step - loss: 1.6829\n",
            "Epoch 14/100\n",
            "134054/134054 [==============================] - 211s 2ms/step - loss: 1.6501\n",
            "Epoch 15/100\n",
            "134054/134054 [==============================] - 215s 2ms/step - loss: 1.6187\n",
            "Epoch 16/100\n",
            "134054/134054 [==============================] - 230s 2ms/step - loss: 1.5896\n",
            "Epoch 17/100\n",
            "134054/134054 [==============================] - 188s 1ms/step - loss: 1.5615\n",
            "Epoch 18/100\n",
            "134054/134054 [==============================] - 145s 1ms/step - loss: 1.5347\n",
            "Epoch 19/100\n",
            "134054/134054 [==============================] - 144s 1ms/step - loss: 1.5151\n",
            "Epoch 20/100\n",
            "134054/134054 [==============================] - 159s 1ms/step - loss: 1.4903\n",
            "\n",
            "Epoch 00020: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v4/July_26/epochs_001_100:020.hdf5\n",
            "Epoch 21/100\n",
            "134054/134054 [==============================] - 265s 2ms/step - loss: 1.4670\n",
            "Epoch 22/100\n",
            "134054/134054 [==============================] - 251s 2ms/step - loss: 1.4426\n",
            "Epoch 23/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.4226\n",
            "Epoch 24/100\n",
            "134054/134054 [==============================] - 296s 2ms/step - loss: 1.4039\n",
            "Epoch 25/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.3847\n",
            "Epoch 26/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.3670\n",
            "Epoch 27/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.3450\n",
            "Epoch 28/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.3263\n",
            "Epoch 29/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.3113\n",
            "Epoch 30/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.2952\n",
            "\n",
            "Epoch 00030: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v4/July_26/epochs_001_100:030.hdf5\n",
            "Epoch 31/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.2788\n",
            "Epoch 32/100\n",
            "134054/134054 [==============================] - 293s 2ms/step - loss: 1.2626\n",
            "Epoch 33/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 1.2435\n",
            "Epoch 34/100\n",
            "134054/134054 [==============================] - 296s 2ms/step - loss: 1.2256\n",
            "Epoch 35/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 1.2128\n",
            "Epoch 36/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 1.1949\n",
            "Epoch 37/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.1820\n",
            "Epoch 38/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.1671\n",
            "Epoch 39/100\n",
            "134054/134054 [==============================] - 293s 2ms/step - loss: 1.1536\n",
            "Epoch 40/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.1401\n",
            "\n",
            "Epoch 00040: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v4/July_26/epochs_001_100:040.hdf5\n",
            "Epoch 41/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.1287\n",
            "Epoch 42/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.1108\n",
            "Epoch 43/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 1.0968\n",
            "Epoch 44/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 1.0842\n",
            "Epoch 45/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 1.0701\n",
            "Epoch 46/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 1.0607\n",
            "Epoch 47/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.0430\n",
            "Epoch 48/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.0286\n",
            "Epoch 49/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 1.0196\n",
            "Epoch 50/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 1.0077\n",
            "\n",
            "Epoch 00050: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v4/July_26/epochs_001_100:050.hdf5\n",
            "Epoch 51/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 0.9955\n",
            "Epoch 52/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.9823\n",
            "Epoch 53/100\n",
            "134054/134054 [==============================] - 293s 2ms/step - loss: 0.9726\n",
            "Epoch 54/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.9678\n",
            "Epoch 55/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.9486\n",
            "Epoch 56/100\n",
            "134054/134054 [==============================] - 293s 2ms/step - loss: 0.9401\n",
            "Epoch 57/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.9258\n",
            "Epoch 58/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.9187\n",
            "Epoch 59/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.9094\n",
            "Epoch 60/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.8943\n",
            "\n",
            "Epoch 00060: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v4/July_26/epochs_001_100:060.hdf5\n",
            "Epoch 61/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 0.8871\n",
            "Epoch 62/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 0.8777\n",
            "Epoch 63/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.8649\n",
            "Epoch 64/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.8538\n",
            "Epoch 65/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.8493\n",
            "Epoch 66/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.8380\n",
            "Epoch 67/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 0.8293\n",
            "Epoch 68/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 0.8225\n",
            "Epoch 69/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 0.8093\n",
            "Epoch 70/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.8047\n",
            "\n",
            "Epoch 00070: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v4/July_26/epochs_001_100:070.hdf5\n",
            "Epoch 71/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.7970\n",
            "Epoch 72/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 0.7887\n",
            "Epoch 73/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 0.7803\n",
            "Epoch 74/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 0.7728\n",
            "Epoch 75/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.7647\n",
            "Epoch 76/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.7580\n",
            "Epoch 77/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.7497\n",
            "Epoch 78/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.7442\n",
            "Epoch 79/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.7401\n",
            "Epoch 80/100\n",
            "134054/134054 [==============================] - 297s 2ms/step - loss: 0.7264\n",
            "\n",
            "Epoch 00080: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v4/July_26/epochs_001_100:080.hdf5\n",
            "Epoch 81/100\n",
            "134054/134054 [==============================] - 296s 2ms/step - loss: 0.7193\n",
            "Epoch 82/100\n",
            "134054/134054 [==============================] - 297s 2ms/step - loss: 0.7187\n",
            "Epoch 83/100\n",
            "134054/134054 [==============================] - 297s 2ms/step - loss: 0.7129\n",
            "Epoch 84/100\n",
            "134054/134054 [==============================] - 297s 2ms/step - loss: 0.7039\n",
            "Epoch 85/100\n",
            "134054/134054 [==============================] - 296s 2ms/step - loss: 0.6971\n",
            "Epoch 86/100\n",
            "134054/134054 [==============================] - 297s 2ms/step - loss: 0.6915\n",
            "Epoch 87/100\n",
            "134054/134054 [==============================] - 299s 2ms/step - loss: 0.6816\n",
            "Epoch 88/100\n",
            "134054/134054 [==============================] - 297s 2ms/step - loss: 0.6806\n",
            "Epoch 89/100\n",
            "134054/134054 [==============================] - 297s 2ms/step - loss: 0.6782\n",
            "Epoch 90/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.6654\n",
            "\n",
            "Epoch 00090: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v4/July_26/epochs_001_100:090.hdf5\n",
            "Epoch 91/100\n",
            "134054/134054 [==============================] - 299s 2ms/step - loss: 0.6623\n",
            "Epoch 92/100\n",
            "134054/134054 [==============================] - 299s 2ms/step - loss: 0.6593\n",
            "Epoch 93/100\n",
            "134054/134054 [==============================] - 301s 2ms/step - loss: 0.6531\n",
            "Epoch 94/100\n",
            "134054/134054 [==============================] - 297s 2ms/step - loss: 0.6438\n",
            "Epoch 95/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 0.6403\n",
            "Epoch 96/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 0.6392\n",
            "Epoch 97/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 0.6300\n",
            "Epoch 98/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 0.6292\n",
            "Epoch 99/100\n",
            "134054/134054 [==============================] - 294s 2ms/step - loss: 0.6296\n",
            "Epoch 100/100\n",
            "134054/134054 [==============================] - 295s 2ms/step - loss: 0.6176\n",
            "\n",
            "Epoch 00100: saving model to /content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v4/July_26/epochs_001_100:100.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lxh67v1HfAUR",
        "colab_type": "text"
      },
      "source": [
        "## Predicting characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aibulx7yfEcz",
        "colab_type": "text"
      },
      "source": [
        "## Predicting 100 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUDPCH8RfDsC",
        "colab_type": "code",
        "outputId": "efea47c6-7bee-40d5-c6e3-b2314f083e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "txt_generator=TextGenerator()\n",
        "txt_generator.load_and_clean_text(downloaded.GetContentString())\n",
        "txt_generator.prepare_data_set(seq_len=100)\n",
        "txt_generator.create_sequence_model(num_lstm_units=256,drop_out=0.1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  135043\n",
            "Total Vocab:  28\n",
            "Total Patterns:  134054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0726 17:45:39.530944 140146659190656 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0726 17:45:39.547890 140146659190656 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0726 17:45:39.550692 140146659190656 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "shape of dataX_padded: (134054, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0726 17:45:39.741150 140146659190656 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0726 17:45:39.754376 140146659190656 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 28)                7196      \n",
            "=================================================================\n",
            "Total params: 796,700\n",
            "Trainable params: 796,700\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZxVEkMzf6XZ",
        "colab_type": "code",
        "outputId": "d6bdeb71-2fb1-4c33-cc08-a4a7aa08388d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "txt_generator.sequence_model.load_weights(\"/content/gdrive/My Drive/EIP/Phase2/Assignment2/weights/approach1_v4/July_26/epochs_001_100:100.hdf5\")\n",
        "txt_generator.sequence_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0726 17:45:47.295643 140146659190656 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0726 17:45:47.895800 140146659190656 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCCTKCxAgIMl",
        "colab_type": "text"
      },
      "source": [
        "## Predicting 500 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBmkI3GKgLwU",
        "colab_type": "code",
        "outputId": "f950d996-f987-47d9-806c-2677d10d340a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "prediction=txt_generator.predict(char_seed='alice wants to',num_chars_to_predict=500)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: alice wants to\n",
            "alice wants tooo the whoug bill re court wie grrtm up shehe ie puertes mooe and the couldnt ceat c t\n",
            "r suy i seaningd ttbj shf what sorring tailath hanlt sfmarked reraeperwed all toundle poe of tome we\n",
            "ry murking about anice whought the her woice of hereing douso atd lad larp besot saee alaume of teal\n",
            "lr said tooe oeart tomethine nook oinutes i tuisting forugh the fouettaii the waid to herself inw tr\n",
            "yisgter whoue noned atdrply all the coust and tomp weryendadouegsed anl tound alain oock turping tfrpiet inge at s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJVUsN9LUR_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "98788064-54b3-4a7f-ef25-ca38c6dc2560"
      },
      "source": [
        "prediction=txt_generator.predict_with_temperature(char_seed='alice wants to',num_chars_to_predict=500,temperature=0.2)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alice wants tooo the whoug bill re court wie grrtm up shehe ie puertes mooe and the couldnt ceat c t\n",
            "r suy i seaningd ttbj shf what sorring tailath hanlt sfmarked reraeperwed all toundle poe of tome we\n",
            "ry murking about anice whought the her woice of hereing douso atd lad larp besot saee alaume of teal\n",
            "lr said tooe oeart tomethine nook oinutes i tuisting forugh the fouettaii the waid to herself inw tr\n",
            "yisgter whoue noned atdrply all the coust and tomp weryendadouegsed anl tound alain oock turping tfrpiet inge at s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIaUuOETUY2j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c62d1b19-992d-4d67-d9a6-2b11fdbb04ab"
      },
      "source": [
        "prediction=txt_generator.predict_with_temperature(char_seed='alice wants to',num_chars_to_predict=500,temperature=0.5)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alice wants tooo the whoug bill re court wie grrtm up shehe ie puertes mooe and the couldnt ceat c t\n",
            "r suy i seaningd ttbj shf what sorring tailath hanlt sfmarked reraeperwed all toundle poe of tome we\n",
            "ry murking about anice whought the her woice of hereing douso atd lad larp besot saee alaume of teal\n",
            "lr said tooe oeart tomethine nook oinutes i tuisting forugh the fouettaii the waid to herself inw tr\n",
            "yisgter whoue noned atdrply all the coust and tomp weryendadouegsed anl tound alain oock turping tfrpiet inge at s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGHg13-jUexy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9dbe6ddb-c5cc-44fb-d9f2-34964e23a6b5"
      },
      "source": [
        "prediction=txt_generator.predict_with_temperature(char_seed='alice wants to',num_chars_to_predict=500,temperature=1.0)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alice wants tooo the whoug bill re court wie grrtm up shehe ie puertes mooe and the couldnt ceat c t\n",
            "r suy i seaningd ttbj shf what sorring tailath hanlt sfmarked reraeperwed all toundle poe of tome we\n",
            "ry murking about anice whought the her woice of hereing douso atd lad larp besot saee alaume of teal\n",
            "lr said tooe oeart tomethine nook oinutes i tuisting forugh the fouettaii the waid to herself inw tr\n",
            "yisgter whoue noned atdrply all the coust and tomp weryendadouegsed anl tound alain oock turping tfrpiet inge at s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDZCDEXBUgLV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4d3f3e42-acb4-4b9f-8954-efa17bc4a7bf"
      },
      "source": [
        "prediction=txt_generator.predict_with_temperature(char_seed='alice wants to',num_chars_to_predict=500,temperature=1.2)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alice wants tooo the whoug bill re court wie grrtm up shehe ie puertes mooe and the couldnt ceat c t\n",
            "r suy i seaningd ttbj shf what sorring tailath hanlt sfmarked reraeperwed all toundle poe of tome we\n",
            "ry murking about anice whought the her woice of hereing douso atd lad larp besot saee alaume of teal\n",
            "lr said tooe oeart tomethine nook oinutes i tuisting forugh the fouettaii the waid to herself inw tr\n",
            "yisgter whoue noned atdrply all the coust and tomp weryendadouegsed anl tound alain oock turping tfrpiet inge at s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja0ZoT1iVGLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52aKq0NSgf3E",
        "colab_type": "code",
        "outputId": "6f545aae-cb41-4faa-e478-624f01d9b1d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "prediction=txt_generator.predict(char_seed='jon snow knows nothing',num_chars_to_predict=500)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: jon snow knows nothing\n",
            "jon snow knows nothing eeat it sather a frcs at well as she could for the fod of the trial dar bette\n",
            "r now his fisst she said to herself in cand one feel cear so she went on all the sat down a large pa\n",
            "bbits were gladrels eoot the walked sather the dourt and the mock turtle said to the duchess and the\n",
            " mocster was a tery diffirotfd of the sane she wery seldmbering wety gnadendhing the queen said the \n",
            "doorman in a parce herself so see iow shan porse ar all the rabbit surtles the walked sather this outping becide she way a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu0LPk4sgi9d",
        "colab_type": "code",
        "outputId": "afd99699-6994-48f1-93ba-9f85d6857c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "prediction=txt_generator.predict(char_seed='a lannister always',num_chars_to_predict=500)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: a lannister always\n",
            "a lannister always get to say the mouse who seemed to be no courtered thought alice without becedent\n",
            "hduieyamd yhat you may gat betadle a baw uaid the duchess the rueen said the duchess as the doumouse\n",
            " looked all round the hatter id you dont know what the mouse goo fighing for a minute or two she was\n",
            " tilence thinking i should the tort of the thimgs is wer she said to herself which way abdeais puite\n",
            " fongotounsy how five to say when i was a little dook and the mock turtle taid alice vhought alice thought it would be\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzxV5ba-gmPM",
        "colab_type": "code",
        "outputId": "d2e6512d-798a-438f-c2ea-da522b5ec05b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "prediction=txt_generator.predict(char_seed='the meaning of life is',num_chars_to_predict=500)\n",
        "print(prediction[0:100])\n",
        "print(prediction[100:200])\n",
        "print(prediction[200:300])\n",
        "print(prediction[300:400])\n",
        "print(prediction[400:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: the meaning of life is\n",
            "the meaning of life is belongs to a mouse shat came vpon a little door kook at well as she could for\n",
            " the fod of the trial dar better now his fisst she said to herself in cand one feel cear so she went\n",
            " on all the sat down a large pabbits were gladrels eoot the walked sather the dourt and the mock tur\n",
            "tle said to the duchess and the mocster was a tery diffirotfd of the sane she wery seldmbering wety \n",
            "gnadendhing the queen said the doorman in a parce herself so see iow shan porse ar all the rabbit surtles the walked sathe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ines19Ogp8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}